{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Sagemaker部署huggingface模型\n",
    "\n",
    "本notebook主要包含以下内容\n",
    "1. 部署huggingface模型到sagemaker推理endpoint\n",
    "2. 使用两种模式使用endpoint进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备工作\n",
    "0. 申请对应机型的配额，通常大致需求如下\n",
    "    * 7B, ml.g5.2xlarge\n",
    "    * 13B, ml.g5.12xlarge\n",
    "    * 70B, ml.g5.24xlarge+\n",
    "1. 创建Sagemaker notebook（比如t3.medium）\n",
    "2. 复制并打开此 notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部署模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'TheBloke/Airoboros-L2-70B-3.1.2-GPTQ',\n",
    "\t'SM_NUM_GPUS': json.dumps(4),\n",
    "    'QUANTIZE': 'gptq'                                      # 根据模型参数配置设置\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.4.2\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.24xlarge\",             # 确保配额足够\n",
    "\tcontainer_startup_health_check_timeout=300,\n",
    "  )\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理\n",
    "\n",
    "#### 使用 predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"Hey my name is Julien! How are you?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 sagemaker runtime调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Sagemaker endpoint 名称\n",
    "endpoint_name = \"huggingface-pytorch-tgi-inference-2024-03-29-08-47-13-951\"                                       # Your endpoint name.\n",
    "content_type = \"application/json\"                                        # The MIME type of the input data in the request body.\n",
    "accept = \"application/json\"                                              # The desired MIME type of the inference in the response.\n",
    "payload = \"\"\"\n",
    "{\n",
    "  \"inputs\": \"Tell me a story about cow boy\",\n",
    "  \"parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40\n",
    "  }\n",
    "}\n",
    "\"\"\"                                          # Payload for inference.\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Accept=accept,\n",
    "    Body=bytes(payload, 'UTF-8')\n",
    "    )\n",
    "\n",
    "result = json.loads(response['Body'].read().decode('UTF-8'))\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
